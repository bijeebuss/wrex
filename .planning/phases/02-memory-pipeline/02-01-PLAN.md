---
phase: 02-memory-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/memory/chunker.ts
  - src/lib/memory/embedder.ts
  - src/lib/memory/types.ts
  - memory/MEMORY.md
autonomous: true

must_haves:
  truths:
    - "Markdown files are split into heading-aware chunks with file path, heading, and line range metadata"
    - "Text is embedded into 768-dimensional vectors using node-llama-cpp with nomic-embed-text-v1.5 Q8_0"
    - "Task prefixes (search_document/search_query) are enforced -- raw text is never embedded directly"
  artifacts:
    - path: "src/lib/memory/types.ts"
      provides: "Chunk and EmbeddedChunk type definitions"
      exports: ["Chunk", "EmbeddedChunk"]
    - path: "src/lib/memory/chunker.ts"
      provides: "Heading-aware markdown chunker"
      exports: ["chunkMarkdown"]
    - path: "src/lib/memory/embedder.ts"
      provides: "Singleton embedding service with task prefix enforcement"
      exports: ["embed", "embedBatch", "getEmbedder"]
    - path: "memory/MEMORY.md"
      provides: "Seed memory file for testing"
      contains: "## "
  key_links:
    - from: "src/lib/memory/embedder.ts"
      to: "node-llama-cpp"
      via: "getLlama -> loadModel -> createEmbeddingContext"
      pattern: "getLlama|loadModel|createEmbeddingContext"
    - from: "src/lib/memory/embedder.ts"
      to: "nomic-embed prefix"
      via: "search_document/search_query prefix enforcement"
      pattern: "search_document:|search_query:"
---

<objective>
Create the markdown chunker and local embedding service that form the input pipeline for the memory system.

Purpose: Memory files need to be split into searchable chunks and converted to 768-dim vectors before they can be indexed. This plan builds those two independent services plus shared types.
Output: Chunker module, embedding service singleton, shared types, seed memory file
</objective>

<execution_context>
@/home/node/.claude/get-shit-done/workflows/execute-plan.md
@/home/node/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-memory-pipeline/02-RESEARCH.md
@src/lib/db/index.ts
@package.json
@tsconfig.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies, download embedding model, and create shared types + markdown chunker</name>
  <files>
    package.json
    src/lib/memory/types.ts
    src/lib/memory/chunker.ts
    memory/MEMORY.md
  </files>
  <action>
1. Install new dependencies:
   ```
   npm install node-llama-cpp @modelcontextprotocol/sdk
   ```

2. Add model download script to package.json scripts:
   ```json
   "models:pull": "npx node-llama-cpp pull --dir ./models hf:nomic-ai/nomic-embed-text-v1.5-GGUF:Q8_0"
   ```
   Also add "models/" to .gitignore if not already present.

3. Run model download:
   ```
   npm run models:pull
   ```
   This downloads the ~140 MiB Q8_0 GGUF model to ./models/. It will take a minute or two. If the download fails due to network issues, add a clear error message in embedder.ts startup check.

4. Create `src/lib/memory/types.ts` with shared types:
   ```typescript
   export interface Chunk {
     content: string;
     filePath: string;
     heading: string;       // nearest parent heading (empty string if none)
     startLine: number;     // 1-indexed
     endLine: number;       // 1-indexed, inclusive
   }

   export interface EmbeddedChunk extends Chunk {
     embedding: number[];   // 768-dimensional float vector
   }

   export const EMBEDDING_DIM = 768;
   ```

5. Create `src/lib/memory/chunker.ts` -- heading-aware markdown chunker:
   - Split on heading boundaries (lines starting with `#`, `##`, or `###`)
   - When a heading is encountered and there is accumulated content, flush the current chunk
   - Track the current heading context (nearest heading above the chunk)
   - Each chunk includes: content (raw text), filePath, heading, startLine, endLine
   - If a section between headings exceeds ~512 tokens (estimate: ~4 chars per token, so ~2048 chars), split it further at paragraph boundaries (double newlines), carrying forward the same heading
   - For sub-chunks from paragraph splitting, keep a small overlap: include the last line of the previous sub-chunk as the first line of the next
   - Export a single function: `chunkMarkdown(content: string, filePath: string): Chunk[]`
   - Handle edge cases: empty file returns [], file with no headings returns single chunk with empty heading, file with only headings returns chunks with heading-only content

6. Create `memory/MEMORY.md` -- a seed memory file for testing:
   ```markdown
   # Wrex Memory

   ## Project Overview
   Wrex is a personal AI assistant with persistent memory. It wraps Claude Code CLI in a web interface and uses a searchable memory system backed by markdown files and semantic search.

   ## Architecture
   The system uses TanStack Start with React for the web UI, better-sqlite3 with sqlite-vec for vector search, and node-llama-cpp for local embeddings. The MCP server exposes memory tools to Claude Code.

   ## Key Decisions
   - Local embeddings only via node-llama-cpp (no external API keys)
   - SQLite for everything (memory index + session storage)
   - Hybrid search combining vector similarity and FTS5 keyword search
   ```
  </action>
  <verify>
    - `npx tsc --noEmit` passes (TypeScript compiles without errors)
    - `ls models/*.gguf` shows the downloaded model file
    - `node -e "import('./src/lib/memory/chunker.ts')"` -- this won't work directly, but TypeScript compilation confirms the module is valid
  </verify>
  <done>
    - types.ts exports Chunk, EmbeddedChunk, and EMBEDDING_DIM (768)
    - chunker.ts exports chunkMarkdown() that splits markdown at heading boundaries with line tracking
    - memory/MEMORY.md exists with at least 3 headings for test data
    - node-llama-cpp and @modelcontextprotocol/sdk installed in package.json
    - nomic-embed-text-v1.5 Q8_0 GGUF model downloaded to models/
  </done>
</task>

<task type="auto">
  <name>Task 2: Create singleton embedding service with task prefix enforcement</name>
  <files>
    src/lib/memory/embedder.ts
  </files>
  <action>
Create `src/lib/memory/embedder.ts` -- singleton embedding service wrapping node-llama-cpp.

Key implementation details:

1. **Singleton pattern:** Use a module-level variable for the embedding context. `getEmbedder()` loads the model on first call and caches it. Loading is expensive (~seconds), embedding is fast once loaded.

2. **Model resolution:** Use `resolveModelFile` from node-llama-cpp to resolve the GGUF path. Use the HuggingFace URI `hf:nomic-ai/nomic-embed-text-v1.5-GGUF:Q8_0` with models dir at `path.join(process.cwd(), "models")`. This handles caching and file resolution. If the model file is not found, throw a clear error directing the user to run `npm run models:pull`.

3. **Task prefix enforcement (CRITICAL):** nomic-embed-text models require task prefixes for correct embedding behavior. Create a wrapper:
   - `embed(text: string, type: "search_document" | "search_query"): Promise<number[]>`
   - The function MUST prepend `search_document: ` or `search_query: ` before the text
   - NEVER expose the raw `getEmbeddingFor()` call outside this module
   - Validate the returned vector length is exactly 768 dimensions

4. **Batch embedding:** Also export `embedBatch(texts: string[], type: "search_document" | "search_query"): Promise<number[][]>` for efficiency when indexing multiple chunks. Use sequential embedding (one at a time) since node-llama-cpp's embedding context handles one request at a time. The function maps over texts and calls embed() for each.

5. **Cleanup export:** Export `disposeEmbedder(): Promise<void>` to release the model resources (call `embeddingContext.dispose()` and set singleton to null). This is needed for clean shutdown of the MCP server.

6. **Error handling:** Wrap model loading in try/catch. If loading fails, provide a clear error message. If embedding fails for a specific text, log the error to stderr and re-throw.

Do NOT use console.log anywhere in this module (it will be used by the MCP server which communicates over stdout). Use console.error for any diagnostics.
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - Run a quick smoke test: `npx tsx -e "import { embed } from './src/lib/memory/embedder.ts'; embed('hello world', 'search_query').then(v => { console.log('dim:', v.length); console.log('first 3:', v.slice(0, 3)); process.exit(0); })"` -- should output dim: 768 and three float values. (This will take a few seconds on first run due to model loading.)
  </verify>
  <done>
    - embedder.ts exports embed(), embedBatch(), getEmbedder(), disposeEmbedder()
    - embed('test', 'search_document') returns a 768-dimensional number array
    - embed('test', 'search_query') returns a 768-dimensional number array
    - No console.log calls in the module (only console.error)
    - Singleton pattern: calling getEmbedder() twice returns the same context
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles cleanly: `npx tsc --noEmit`
2. Model file exists: `ls -la models/*.gguf` shows ~140 MiB file
3. Chunker works: Import chunkMarkdown, pass memory/MEMORY.md content, verify it returns 3+ chunks with correct heading/line metadata
4. Embedder works: Import embed, embed a test string, verify 768-dim output
5. Task prefix enforced: embed function signature requires type parameter
</verification>

<success_criteria>
- Markdown chunker splits at headings, tracks line numbers, handles edge cases
- Embedding service loads nomic-embed Q8_0 model and produces 768-dim vectors
- Task prefixes (search_document/search_query) are enforced at the API level
- Both modules are importable with clean TypeScript types
</success_criteria>

<output>
After completion, create `.planning/phases/02-memory-pipeline/02-01-SUMMARY.md`
</output>
